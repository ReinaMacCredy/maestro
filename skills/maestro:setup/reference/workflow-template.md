# Workflow Configuration

> Source of truth for how `/maestro:implement` executes tasks.
> Generated by `/maestro:setup`. Edit directly to customize.

## Methodology
{methodology}

### TDD Lifecycle (if TDD selected)
1. **Red**: Write failing test that defines expected behavior
2. **Green**: Write minimum code to make the test pass
3. **Refactor**: Improve code quality with green tests as safety net
4. Rerun tests after refactor to confirm still passing

### Ship-fast Lifecycle (if ship-fast selected)
1. **Implement**: Write the feature/fix code
2. **Test**: Add tests covering the implementation
3. **Verify**: Run tests to confirm passing

## Quality Targets
- **Test coverage**: {coverage_target}
- **CI-aware**: Use `CI=true` prefix for watch-mode tools (jest, vitest, etc.)

## Quality Gates Checklist

Before marking any phase as complete, verify ALL of the following:

- [ ] All new code has corresponding tests
- [ ] Test coverage meets or exceeds threshold ({coverage_target})
- [ ] No new lint warnings or errors
- [ ] Type checking passes (if applicable)
- [ ] No hardcoded secrets, tokens, or credentials
- [ ] Error handling covers expected failure modes
- [ ] Input validation at all system boundaries
- [ ] API contracts match specification
- [ ] UI changes are accessible (keyboard nav, screen reader, contrast)
- [ ] Mobile responsiveness verified (if applicable)
- [ ] Documentation updated for public API changes
- [ ] No TODO/FIXME items left unresolved in changed files

## Testing Requirements

### Unit Tests
- Pure logic and transformations
- Edge cases and boundary values
- Error paths and exception handling

### Integration Tests
- Database operations
- External API interactions
- File system operations
- Cross-module communication

### Mobile/UI Tests (if applicable)
- Touch targets meet minimum size (44x44pt)
- Orientation changes handled
- Offline/slow network behavior
- Accessibility labels present

## Commit Strategy
- **Frequency**: {commit_frequency}
- **Format**: `{type}({scope}): {description}` (conventional commits)
- **Summary storage**: {summary_storage}

### Commit Types
| Type | Use When |
|------|----------|
| `feat` | New feature or capability |
| `fix` | Bug fix |
| `docs` | Documentation changes only |
| `style` | Formatting, no code change |
| `refactor` | Code restructuring, no behavior change |
| `test` | Adding or updating tests only |
| `chore` | Build, tooling, infrastructure |

### Git Notes Format (if git notes selected)
```
Task: {task_name}
Phase: {phase_number}
Changes: {files_changed}
Summary: {what_and_why}
```

Command: `git notes add -m "{note}" {commit_hash}`

## Self-Review Checklist

Before committing, verify:

### Functionality
- [ ] Code does what the spec requires
- [ ] Edge cases handled
- [ ] No regressions in existing behavior

### Code Quality
- [ ] Naming is clear and consistent
- [ ] No unnecessary complexity
- [ ] No code duplication
- [ ] Functions are focused (single responsibility)

### Testing
- [ ] Tests cover the change
- [ ] Tests are deterministic (no flaky tests)
- [ ] Test names describe expected behavior

### Security
- [ ] No SQL/command injection vectors
- [ ] No XSS vectors (if web)
- [ ] Auth/authz checks present where needed
- [ ] Sensitive data not logged

### Performance
- [ ] No N+1 queries
- [ ] No unbounded loops or recursion
- [ ] Large data sets paginated

## Phase Completion Protocol
1. **Coverage check**: Run coverage for files changed since last checkpoint
2. **Test execution**: Run full test suite (`CI=true`)
   - Max 2 fix attempts on failure
   - If still failing: halt and ask user
3. **Manual verification**: Present step-by-step verification plan to user
4. **User confirmation**: Wait for explicit approval before next phase

## Definition of Done

A task/phase/track is "done" when ALL of the following are true:

1. Implementation matches the acceptance criteria in spec.md
2. All tests pass (unit + integration)
3. Coverage meets threshold ({coverage_target})
4. No lint errors or type errors
5. Code reviewed (self-review checklist completed)
6. Commit(s) created with proper conventional commit messages
7. Plan.md updated with completion SHA
8. Manual verification steps confirmed by user
9. No unresolved TODOs in changed code

## Tech Stack Deviation Protocol
If implementation requires a technology not listed in `tech-stack.md`:
1. STOP implementation
2. Update `tech-stack.md` with: new technology, rationale, date
3. Get user confirmation
4. Resume implementation

## Development Commands

Populate these during `/maestro:setup` based on the detected tech stack:

```bash
# Setup
{setup_command}          # e.g., npm install, pip install -e ., cargo build

# Daily development
{dev_command}            # e.g., npm run dev, python manage.py runserver
{test_command}           # e.g., npm test, pytest, cargo test
{lint_command}           # e.g., npm run lint, ruff check ., cargo clippy
{typecheck_command}      # e.g., npx tsc --noEmit, mypy .
{coverage_command}       # e.g., npm test -- --coverage, pytest --cov

# Pre-commit
{format_command}         # e.g., npm run format, black ., cargo fmt
```

## Emergency Procedures

### Critical Bug in Production
1. Assess severity and user impact
2. If data loss possible: notify stakeholders immediately
3. Create a hotfix track: `/maestro:new-track "Hotfix: {description}"`
4. Implement the minimum fix (skip full TDD if time-critical, but add tests after)
5. Deploy via expedited path
6. Post-mortem: add regression test, update guidelines if needed

### Security Breach
1. Rotate any compromised credentials immediately
2. Assess exposure scope
3. Create a security track with highest priority
4. Implement fix + audit related code
5. Document in guidelines.md

### Data Loss
1. Stop the affected process
2. Assess recovery options (backups, transaction logs)
3. Create a recovery track
4. Implement fix with additional safeguards
5. Add monitoring/alerting

## Continuous Improvement

After completing each track, consider:
- What went well? What was painful?
- Were the estimates accurate? (adjust sizing guidelines)
- Any new patterns worth extracting to style guides?
- Any new tools or dependencies worth documenting?

Record insights in `.maestro/wisdom/` via `/maestro:implement`'s wisdom extraction.

## Deployment Workflow (if applicable)

### Pre-Deployment
- [ ] All tests pass on CI
- [ ] No critical/high severity review findings
- [ ] Database migrations tested (if applicable)
- [ ] Environment variables configured
- [ ] Rollback plan documented

### Deployment Steps
1. Merge to deployment branch
2. Run CI pipeline
3. Deploy to staging
4. Smoke test on staging
5. Deploy to production
6. Verify production health

### Post-Deployment
- [ ] Health checks passing
- [ ] Error rates normal
- [ ] Performance metrics within bounds
- [ ] User-facing flows verified
